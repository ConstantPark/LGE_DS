{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**단계 1:** 패키지를 import 하는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아이리스 데이터가 iris 변수에 저장이 되었는지를 확인합니다.  \n",
    "load_iris() 함수는 아이리스 dataset을 불러오는 기능을 수행합니다.  \n",
    "iris = load_iris()는 불러온 아이리스 데이터를 iris라는 변수에 할당합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**단계 2:** iris 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris=load_iris()\n",
    "iris.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iris.keys()는 iris data set의 컬럼 값을 출력하는 함수입니다.  \n",
    "Iris data set의 컬럼 값으로 data, feature_names, target, target_names, DESCR 등이 있는 것을 볼 수 있습니다.\n",
    "\n",
    "data 컬럼은 아이리스 데이터가 있는 부분이며, feature_names는 iris data set의 특징을 기술하는 부분입니다.  \n",
    "Target과 target_names는 다층신경망을 이용해 분석하려는 목적변수이며 target에는 setosa, verginia, virginica 등이 있습니다.  \n",
    "마지막으로 DESCR은 iris data에 대한 설명을 포함하고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['data'][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iris data 0번째부터 9번째까지를 슬라이싱 합니다.\n",
    "10개의 배열이 출력되는데, 이 값들은 아이리스 데이터를 분류하는 특징 값을 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**단계 3:** 데이터 셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris['data']\n",
    "y = iris['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris data set의 data를 X변수에 할당하며 분류하고자 하는 target 변수는 y변수에 할당합니다.  \n",
    "즉, y변수에는 iris 종, 즉 setosa, versicolor,virginica등이 들어있으며, X에는 해당 종에 대한 특징 값들이 들어 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**단계 4:** 학습과 테스트 셋 구분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn.model_selection 라이브러리 안에 있는 train_test_split 모듈을 이용합니다.  \n",
    "sklearn.model_selection의 train_test_split 모듈을 import하여, \n",
    "train_test_split 모듈을 이용해 Training Data Set은 75%, Test Data Set은 25%로 분할 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분할된 훈련 데이터셋과 테스트 데이터셋은 각각 X_train, X_test, y_train, y_test에 할당됩니다.\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**단계 5:** 데이터 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = ?()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn라이브러리의 preprocessing 모듈에 있는 StandardScaler 함수를 import하고, \n",
    "StandardScaler 함수의 결과 값을 scaler라는 변수에 저정합니다.  \n",
    "이 StandardScaler 함수는 데이터의 범위를 평균 0, 표준편차 1의 범위로 바꿔주는 모듈입니다.  \n",
    "결과적으로 scaler 변수에는 StandardScaler 함수를 통해 정규화된 데이터가 저장됩니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train에 대한 데이터를 scaler.fit()을 이용해 평균과 표준편차를 계산합니다.\n",
    "scaler.?(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test 데이터 정규화 및 변환\n",
    "# transform은 데이터를 정규화 형식으로 변환합니다.\n",
    "X_train = scaler.?(X_train)\n",
    "X_test = scaler.?(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**단계 6:** MLP 알고리즘 로드 및 Hidden Layer 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.neural_network 라이브러리의 MLPClassifier라는 모듈을 import합니다.\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLPClassifier는 다중신경망 분류 알고리즘을 저장하고 있는 모듈\n",
    "# mlp라는 변수에 MLPClassifier() 함수를 실행한 결과를 저장합니다.\n",
    "# 함수의 파라미터로 hidden_layer_sizes=(10,10,10)과 같이 설정했는데\n",
    "# 이것은 3개의 은닉층을 만들고 각 계층별로 10개의 노드씩 할당하라는 의미입니다 !\n",
    "mlp = ?(?=(10,10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(10, 10, 10), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation : 다층 신경망에서 사용하는 활성화 함수\n",
    "Alpha : 신경망 내의 정규화 파라미터\n",
    "Batch_size : 최적화를 시키기 위한 학습 최소 크기\n",
    "Epsilon : 수치 안정성을 위한 오차 값\n",
    "Learning_rate_init : 가중치를 업데이트 할 때 크기를 제어\n",
    "Max_iter : 최대 반복 횟수\n",
    "Hidden_layer_sizes : 히든 레이어의 크기\n",
    "Learning_rate : 단계별로 움직이는 학습 속도\n",
    "Shuffle : 데이터를 학습 시 데이터들의 위치를 임의적으로 변경하는 지의 여부\n",
    "Solver : 가중치 최적화를 위해 사용하는 함수\n",
    "Validation_fraction : training data를 학습 시 validation의 비율\n",
    "Validation : training data를 학습 시 데이터가 유의미한지를 검증하는 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측의 결과를 predicions 변수에 저장\n",
    "predictions = mlp.?(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**단계 7:** 학습의 성능을 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가를 위해 classification_report와 confusion_matrix 모듈을 사용합니다.\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17  0  0]\n",
      " [ 0  8  5]\n",
      " [ 0  3  5]]\n"
     ]
    }
   ],
   "source": [
    "print(?(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과 값으로 [17 0 0]이라는 것을 볼수 있는데, 이것은 17개가 정상적으로 분류되었다는 의미입니다.  \n",
    "두번째 배열 [0 8 5]의 경우 vericolor라는 클래스는 8개를 분류했지만\n",
    "5개의 경우 예측 값이 잘못되어 versicolor를 virginica로 분류한 경우입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        17\n",
      "           1       0.73      0.62      0.67        13\n",
      "           2       0.50      0.62      0.56         8\n",
      "\n",
      "    accuracy                           0.79        38\n",
      "   macro avg       0.74      0.75      0.74        38\n",
      "weighted avg       0.80      0.79      0.79        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(?(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification_report모듈에서는 Precision(정확률), Recall(재현률), f1-score를 계산합니다.\n",
    "실제 결과와 예측 결과를 비교했을 때 둘 다 참인 경우를 TP(True Positive)라고 하며 \n",
    "실제 결과는 참이지만 예측 결과가 거짓인 경우를 FN(False Negative)라 합니다.\n",
    "또한 실제 결과는 거짓인데 예측 결과에서 참인 경우를 FP(False Positive)라 하며, 둘 다 거짓인 경우를 TN 즉, True Negative라 합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
